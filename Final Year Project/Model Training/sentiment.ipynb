{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Automatic_Ticket_Assignment.xlsx')\n",
    "df.columns=['ShortDescription','Description', 'Caller', 'AssignmentGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = df['AssignmentGroup'].value_counts()\n",
    "to_remove = group_counts[group_counts < 200].index\n",
    "df = df[~df['AssignmentGroup'].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df.apply(lambda row: row['ShortDescription'] if pd.isna(row['Description']) else row['Description'], axis=1)\n",
    "df['ShortDescription'] = df.apply(lambda row: row['Description'] if pd.isna(row['ShortDescription']) else row['ShortDescription'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def apply_contractions(text):\n",
    "    new_phrase = []\n",
    "    for word in text.split():\n",
    "        new_phrase.append(contractions.fix(word))\n",
    "        \n",
    "    return ' '.join(new_phrase)\n",
    "        \n",
    "# Expanding Contractions in the reviews\n",
    "df['Description']=df['Description'].apply(lambda x:apply_contractions(x))\n",
    "df['ShortDescription']=df['ShortDescription'].apply(lambda x:apply_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text):\n",
    "    #converting to lowercase\n",
    "    newString = text.lower()\n",
    "    #removing links\n",
    "    newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString) \n",
    "    #fetching alphabetic characters\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    #removing stop words\n",
    "    tokens = [w for w in newString.split() if not w in stop_words] \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        #removing short words\n",
    "        if len(i)>1:                                                 \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['Description']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['Cleaned_Description'] = cleaned_text\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['ShortDescription']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['ShortDescription'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Consolidated_Text']=df['Cleaned_Description'] + ' ' + df['ShortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# function to genarate word tokens for tokenizers\n",
    "\n",
    "def tokenization_func(text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "list_for_sentence_word_tokens = []\n",
    "\n",
    "for sen in df.Consolidated_Text:\n",
    "    list_for_sentence_word_tokens.append(tokenization_func(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part Of Speech Tagging\n",
    "\n",
    "list_of_sen_with_part_of_speech_tagging = []\n",
    "for sen_list in list_for_sentence_word_tokens:\n",
    "    list_of_sen_with_part_of_speech_tagging.append(nltk.pos_tag(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Map POS tags to wordnet tags\n",
    "# This step is necessary because the lemmatizer requires WordNet tags instead of POS tags\n",
    "wordnet_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Normalize the words using lemmatization with the appropriate POS tags\n",
    "list_of_lemmatized_sen = []\n",
    "for sen_list in list_of_sen_with_part_of_speech_tagging:\n",
    "    lemmas = []\n",
    "    for word, pos in sen_list:\n",
    "        if pos[0] in wordnet_tags:\n",
    "            tag = wordnet_tags[pos[0]]\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            lemmas.append(lemma)\n",
    "        else:\n",
    "            lemmas.append(word)\n",
    "\n",
    "    # Join the lemmas back into a normalized sentence\n",
    "    normalized_sentence = \" \".join(lemmas)\n",
    "    # insert the lemmatized(normalized_sentence) sentence in a new list called list_of_lemmatized_sen\n",
    "    list_of_lemmatized_sen.append(normalized_sentence)\n",
    "\n",
    "df['Consolidated_Text'] = list_of_lemmatized_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['Consolidated_Text','AssignmentGroup']]\n",
    "new_df.columns=['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def labelencoder(dataframe) : \n",
    "  label_encoder = preprocessing.LabelEncoder() \n",
    "  dataframe= label_encoder.fit_transform(dataframe)\n",
    "  \n",
    "  return dataframe\n",
    "\n",
    "new_df['label'] = labelencoder(new_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Devesh\n",
      "[nltk_data]     Udhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Assuming your DataFrame is called new_df\n",
    "new_text = []\n",
    "for text in new_df['text']:\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Determine the highest sentiment category\n",
    "    max_sentiment = max(sentiment_score, key=sentiment_score.get)\n",
    "    \n",
    "    # Map the sentiment category to its label\n",
    "    if max_sentiment == 'neg':\n",
    "        sentiment_label = 'negative'\n",
    "    elif max_sentiment == 'neu':\n",
    "        sentiment_label = 'neutral'\n",
    "    else:\n",
    "        sentiment_label = 'positive'\n",
    "    \n",
    "    # Append the sentiment label to the text\n",
    "    text_with_sentiment = f\"{sentiment_label} {text}\"\n",
    "    new_text.append(text_with_sentiment)\n",
    "#     print(text_with_sentiment)\n",
    "\n",
    "# print(sentiments_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['text'] = new_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
