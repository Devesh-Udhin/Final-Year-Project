{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Automatic_Ticket_Assignment.xlsx')\n",
    "df.columns=['ShortDescription','Description', 'Caller', 'AssignmentGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = df['AssignmentGroup'].value_counts()\n",
    "to_remove = group_counts[group_counts < 200].index\n",
    "df = df[~df['AssignmentGroup'].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df.apply(lambda row: row['ShortDescription'] if pd.isna(row['Description']) else row['Description'], axis=1)\n",
    "df['ShortDescription'] = df.apply(lambda row: row['Description'] if pd.isna(row['ShortDescription']) else row['ShortDescription'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def apply_contractions(text):\n",
    "    new_phrase = []\n",
    "    for word in text.split():\n",
    "        new_phrase.append(contractions.fix(word))\n",
    "        \n",
    "    return ' '.join(new_phrase)\n",
    "        \n",
    "# Expanding Contractions in the reviews\n",
    "df['Description']=df['Description'].apply(lambda x:apply_contractions(x))\n",
    "df['ShortDescription']=df['ShortDescription'].apply(lambda x:apply_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text):\n",
    "    #converting to lowercase\n",
    "    newString = text.lower()\n",
    "    #removing links\n",
    "    newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString) \n",
    "    #fetching alphabetic characters\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    #removing stop words\n",
    "    tokens = [w for w in newString.split() if not w in stop_words] \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        #removing short words\n",
    "        if len(i)>1:                                                 \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['Description']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['Cleaned_Description'] = cleaned_text\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['ShortDescription']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['ShortDescription'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Consolidated_Text']=df['Cleaned_Description'] + ' ' + df['ShortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# function to genarate word tokens for tokenizers\n",
    "\n",
    "def tokenization_func(text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "list_for_sentence_word_tokens = []\n",
    "\n",
    "for sen in df.Consolidated_Text:\n",
    "    list_for_sentence_word_tokens.append(tokenization_func(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part Of Speech Tagging\n",
    "\n",
    "list_of_sen_with_part_of_speech_tagging = []\n",
    "for sen_list in list_for_sentence_word_tokens:\n",
    "    list_of_sen_with_part_of_speech_tagging.append(nltk.pos_tag(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Map POS tags to wordnet tags\n",
    "# This step is necessary because the lemmatizer requires WordNet tags instead of POS tags\n",
    "wordnet_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Normalize the words using lemmatization with the appropriate POS tags\n",
    "list_of_lemmatized_sen = []\n",
    "for sen_list in list_of_sen_with_part_of_speech_tagging:\n",
    "    lemmas = []\n",
    "    for word, pos in sen_list:\n",
    "        if pos[0] in wordnet_tags:\n",
    "            tag = wordnet_tags[pos[0]]\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            lemmas.append(lemma)\n",
    "        else:\n",
    "            lemmas.append(word)\n",
    "\n",
    "    # Join the lemmas back into a normalized sentence\n",
    "    normalized_sentence = \" \".join(lemmas)\n",
    "    # insert the lemmatized(normalized_sentence) sentence in a new list called list_of_lemmatized_sen\n",
    "    list_of_lemmatized_sen.append(normalized_sentence)\n",
    "\n",
    "df['Consolidated_Text'] = list_of_lemmatized_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['Consolidated_Text','AssignmentGroup']]\n",
    "new_df.columns=['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def labelencoder(dataframe) : \n",
    "  label_encoder = preprocessing.LabelEncoder() \n",
    "  dataframe= label_encoder.fit_transform(dataframe)\n",
    "  \n",
    "  return dataframe\n",
    "\n",
    "new_df['label'] = labelencoder(new_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10193 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [line.split(' ') for line in new_df['text']]\n",
    "word2vec = Word2Vec(sentences=sentences,min_count=1) # min_count=1 means that we are considering all the words in the corpus\n",
    "# This file will be used later to load the embeddings into memory for training a neural network\n",
    "# By default each word will be represented by a 100 dimensional vector\n",
    "word2vec.wv.save_word2vec_format('word2vec_vector.txt')\n",
    "\n",
    "# load the whole embedding\n",
    "embeddings_index = dict()\n",
    "f = open('word2vec_vector.txt')\n",
    "\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32') # converts the string vectors to float and store in a numpy array\n",
    "\tembeddings_index[word] = coefs # store the word and its corresponding vector in a dictionary\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "maxlen = 100\n",
    "numWords=10000\n",
    "epochs = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedModelData:\n",
    "    \n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "    embedding_matrix=[]\n",
    "\n",
    "    # 1\n",
    "    def getData(self, dataframe):\n",
    "\n",
    "      X,y = self.tokenizeAndEmbedding(dataframe)\n",
    "      X_train, X_test, y_train, y_test, X_Val, y_Val = self.splitData(X,y)      \n",
    " \n",
    "      return X_train, X_test, y_train, y_test, X_Val, y_Val, self.embedding_matrix\n",
    "  \n",
    "    # 2\n",
    "    def tokenizeAndEmbedding(self,dataframe):\n",
    "\n",
    "      tokenizer,X = self.wordTokenizer(dataframe['text'])\n",
    "      y = np.asarray(dataframe['label'])\n",
    "      X = pad_sequences(X, maxlen = maxlen, padding='post', truncating='post') # ensure that all sequences have the same length                \n",
    "      self.embedding_matrix = np.zeros((numWords+1, 100))\n",
    "\n",
    "      for i,word in tokenizer.index_word.items():\n",
    "        if i<numWords+1: # we are taking only the first 9000 words\n",
    "          embedding_vector = embeddings_index.get(word)\n",
    "          if embedding_vector is not None:\n",
    "              self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "      return X,y\n",
    "    \n",
    "    # 3\n",
    "    def wordTokenizer(self, dataframe):\n",
    "      tokenizer = Tokenizer(num_words=numWords, lower=True, split=' ', char_level=False)\n",
    "      tokenizer.fit_on_texts(dataframe) # convert each word in the text into a unique integer ID\n",
    "      dataframe = tokenizer.texts_to_sequences(dataframe) # transform each text in dataframe into a sequence of integer indices\n",
    "      \n",
    "      with open('tokenizer.pickle', 'wb') as handle:\n",
    "          pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "      return tokenizer,dataframe\n",
    "\n",
    "    # 4\n",
    "    def splitData(self,X,y):\n",
    "\n",
    "      X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=10)     \n",
    "      X_train, X_Val, y_train, y_Val = train_test_split(X_train, y_train, test_size=0.1, random_state=10)\n",
    "      \n",
    "      #k-fold cross validation\n",
    "      # check classes count\n",
    "             \n",
    "      # smote = SMOTE(random_state=42)\n",
    "      # X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)     \n",
    "      # return X_train_resampled, self.X_test, y_train_resampled, self.y_test, X_Val, y_Val\n",
    "      return X_train, self.X_test, y_train, self.y_test, X_Val, y_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel:\n",
    "    \n",
    "    model= Model()\n",
    "    \n",
    "    def train(self, X_train, y_train, X_Val, y_Val, embedding_matrix, batch_size, epochs):\n",
    "        \n",
    "      input_layer = Input(shape=(maxlen,), dtype=tf.int64)\n",
    "      embed = Embedding(numWords+1, output_dim=100, input_length=maxlen, weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "      \n",
    "      lstm = LSTM(128, return_sequences=True)(embed)\n",
    "      lstm = LSTM(64)(lstm)\n",
    "      \n",
    "      dense = Dense(128, activation='relu')(lstm)\n",
    "      drop=Dropout(0.5)(dense)    \n",
    "      \n",
    "      out = Dense(len((pd.Series(y_train)).unique()), activation='softmax')(drop) \n",
    "      self.model = Model(input_layer, out)    \n",
    "      self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "      self.model.summary()\n",
    "  \n",
    "      reduceLoss = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001)\n",
    "      \n",
    "      model_history = self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduceLoss], validation_data=(X_Val,y_Val))\n",
    "  \n",
    "      return model_history, self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \n",
    "    def prediction(self, model, X_test, y_test):\n",
    "      \n",
    "      pred = model.predict(X_test)\n",
    "      df_pred = pd.DataFrame(pred, columns=['tech1', 'tech2', 'tech3', 'tech4', 'tech5', 'tech6', 'tech7', 'tech8'])\n",
    "      pred = [i.argmax() for i in pred]\n",
    "      \n",
    "      accuracy = metrics.accuracy_score(y_test, pred)\n",
    "      precision = metrics.precision_score(y_test, pred, average='weighted', zero_division=1)\n",
    "      recall = metrics.recall_score(y_test, pred, average='weighted')\n",
    "      f1score = metrics.f1_score(y_test, pred, average='weighted')\n",
    "      \n",
    "      print(\"Precision of Recurrent Neural Network model: \", precision)\n",
    "      print(\"Recall of Recurrent Neural Network model: \", recall)\n",
    "      print(\"F1-Score of Recurrent Neural Network model: \", f1score)\n",
    "      print(\"Accuracy of Recurrent Neural Network model:\", accuracy)\n",
    "      \n",
    "      return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x_train, x_test, y_train, y_test, x_Val, y_Val, embedding_matrix to feed the model\n",
    "FeedModelData = FeedModelData()\n",
    "x_train, x_test, y_train, y_test, x_Val, y_Val, embedding_matrix = FeedModelData.getData(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RNNModel' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Define and train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m RNNModel \u001b[39m=\u001b[39m RNNModel()\n\u001b[0;32m      3\u001b[0m model_history, model \u001b[39m=\u001b[39m RNNModel\u001b[39m.\u001b[39mtrain(x_train, y_train, x_Val, y_Val, embedding_matrix, batch_size, epochs)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'RNNModel' object is not callable"
     ]
    }
   ],
   "source": [
    "# Define and train the model\n",
    "RNNModel = RNNModel()\n",
    "model_history, model = RNNModel.train(x_train, y_train, x_Val, y_Val, embedding_matrix, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 2s 66ms/step\n",
      "Precision of Recurrent Neural Network model:  0.7627380095840988\n",
      "Recall of Recurrent Neural Network model:  0.6923076923076923\n",
      "F1-Score of Recurrent Neural Network model:  0.5852189398577361\n",
      "Accuracy of Recurrent Neural Network model: 0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "# test model on unseen data and get prediction array\n",
    "Prediction = Prediction()\n",
    "all_predictions = Prediction.prediction(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('RNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.to_csv('RNN_model_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
