{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Automatic_Ticket_Assignment.xlsx')\n",
    "df.columns=['ShortDescription','Description', 'Caller', 'AssignmentGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = df['AssignmentGroup'].value_counts()\n",
    "to_remove = group_counts[group_counts < 200].index\n",
    "df = df[~df['AssignmentGroup'].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df.apply(lambda row: row['ShortDescription'] if pd.isna(row['Description']) else row['Description'], axis=1)\n",
    "df['ShortDescription'] = df.apply(lambda row: row['Description'] if pd.isna(row['ShortDescription']) else row['ShortDescription'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def apply_contractions(text):\n",
    "    new_phrase = []\n",
    "    for word in text.split():\n",
    "        new_phrase.append(contractions.fix(word))\n",
    "        \n",
    "    return ' '.join(new_phrase)\n",
    "        \n",
    "# Expanding Contractions in the reviews\n",
    "df['Description']=df['Description'].apply(lambda x:apply_contractions(x))\n",
    "df['ShortDescription']=df['ShortDescription'].apply(lambda x:apply_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text):\n",
    "    #converting to lowercase\n",
    "    newString = text.lower()\n",
    "    #removing links\n",
    "    newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString) \n",
    "    #fetching alphabetic characters\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    #removing stop words\n",
    "    tokens = [w for w in newString.split() if not w in stop_words] \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        #removing short words\n",
    "        if len(i)>1:                                                 \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['Description']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['Cleaned_Description'] = cleaned_text\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['ShortDescription']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['ShortDescription'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Consolidated_Text']=df['Cleaned_Description'] + ' ' + df['ShortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# function to genarate word tokens for tokenizers\n",
    "\n",
    "def tokenization_func(text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "list_for_sentence_word_tokens = []\n",
    "\n",
    "for sen in df.Consolidated_Text:\n",
    "    list_for_sentence_word_tokens.append(tokenization_func(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part Of Speech Tagging\n",
    "\n",
    "list_of_sen_with_part_of_speech_tagging = []\n",
    "for sen_list in list_for_sentence_word_tokens:\n",
    "    list_of_sen_with_part_of_speech_tagging.append(nltk.pos_tag(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Map POS tags to wordnet tags\n",
    "# This step is necessary because the lemmatizer requires WordNet tags instead of POS tags\n",
    "wordnet_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Normalize the words using lemmatization with the appropriate POS tags\n",
    "list_of_lemmatized_sen = []\n",
    "for sen_list in list_of_sen_with_part_of_speech_tagging:\n",
    "    lemmas = []\n",
    "    for word, pos in sen_list:\n",
    "        if pos[0] in wordnet_tags:\n",
    "            tag = wordnet_tags[pos[0]]\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            lemmas.append(lemma)\n",
    "        else:\n",
    "            lemmas.append(word)\n",
    "\n",
    "    # Join the lemmas back into a normalized sentence\n",
    "    normalized_sentence = \" \".join(lemmas)\n",
    "    # insert the lemmatized(normalized_sentence) sentence in a new list called list_of_lemmatized_sen\n",
    "    list_of_lemmatized_sen.append(normalized_sentence)\n",
    "\n",
    "df['Consolidated_Text'] = list_of_lemmatized_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['Consolidated_Text','AssignmentGroup']]\n",
    "new_df.columns=['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def labelencoder(dataframe) : \n",
    "  label_encoder = preprocessing.LabelEncoder() \n",
    "  dataframe= label_encoder.fit_transform(dataframe)\n",
    "  \n",
    "  return dataframe\n",
    "\n",
    "new_df['label'] = labelencoder(new_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    3976\n",
       "6     661\n",
       "4     289\n",
       "1     257\n",
       "7     252\n",
       "3     241\n",
       "2     215\n",
       "5     200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10193 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [line.split(' ') for line in new_df['text']]\n",
    "word2vec = Word2Vec(sentences=sentences,min_count=1) # min_count=1 means that we are considering all the words in the corpus\n",
    "# This file will be used later to load the embeddings into memory for training a neural network\n",
    "# By default each word will be represented by a 100 dimensional vector\n",
    "word2vec.wv.save_word2vec_format('word2vec_vector.txt')\n",
    "\n",
    "# load the whole embedding\n",
    "embeddings_index = dict()\n",
    "f = open('word2vec_vector.txt')\n",
    "\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32') # converts the string vectors to float and store in a numpy array\n",
    "\tembeddings_index[word] = coefs # store the word and its corresponding vector in a dictionary\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "_1, X_test, _2, y_test = train_test_split(new_df['text'], new_df['label'], test_size=0.2, random_state=10)     \n",
    "_1, X_Val, _2, y_Val = train_test_split(_1, _2, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train and y_train into DataFrames\n",
    "train_df = pd.DataFrame({'text': _1, 'label': _2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "\n",
    "label_counts = new_df['label'].value_counts()\n",
    "\n",
    "# Find the majority class (class with the highest count)\n",
    "majority_class = label_counts.idxmax()\n",
    "\n",
    "# Calculate the augmentation ratio for each class\n",
    "augmentation_ratio = {}\n",
    "for label, count in label_counts.items():\n",
    "    if label != majority_class:\n",
    "        ratio = int(label_counts[majority_class] / count)\n",
    "        augmentation_ratio[label] = ratio\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Initialize the augmenter\n",
    "augmenter = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"insert\"\n",
    ")\n",
    "\n",
    "import time\n",
    "count = 0\n",
    "\n",
    "new_train_df = train_df[~train_df['label'].isin([majority_class])]\n",
    "\n",
    "# Augment the data\n",
    "for i, (label, text) in enumerate(zip(new_train_df['label'], new_train_df['text'])):\n",
    "    start_time = time.time()\n",
    "    count += 1\n",
    "    print(\"we are at index : \", count)\n",
    "    augmented_text = text\n",
    "    \n",
    "    if label in augmentation_ratio and count <= new_train_df.count():\n",
    "        ratio = augmentation_ratio[label]\n",
    "        augmented_text = augmenter.augment(augmented_text, n=int(ratio))\n",
    "        new_row = pd.DataFrame({'text': augmented_text, 'label': label})\n",
    "        train_df = pd.concat([train_df, new_row], ignore_index=True)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Average time per augmented sentence:\", execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('Augmented_train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this one for demo because generating augmented text takes too much time\n",
    "train_df = pd.read_csv('Augmented_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "maxlen = 128\n",
    "numWords=10000\n",
    "epochs = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=numWords, lower=True, split=' ', char_level=False)\n",
    "tokenizer.fit_on_texts(new_df['text'])\n",
    "\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_Val = tokenizer.texts_to_sequences(X_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = maxlen, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test, maxlen = maxlen, padding='post', truncating='post')\n",
    "X_Val = pad_sequences(X_Val, maxlen = maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_df['label'])\n",
    "y_test = np.asarray(y_test)\n",
    "y_Val = np.asarray(y_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=[]\n",
    "\n",
    "embedding_matrix = np.zeros((numWords+1, 100))\n",
    "\n",
    "for i,word in tokenizer.index_word.items():\n",
    "  if i<numWords+1: # we are taking only the first 9000 words\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "class CNNModel:\n",
    "\n",
    "    def objective(self, trial):\n",
    "        # Define the search space for hyperparameters\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        num_filters = trial.suggest_int('num_filters', 128, 512)\n",
    "        filters_size = trial.suggest_int('filters_size', 1, 30)\n",
    "        pool_size = trial.suggest_int('pool_size', 1, 30)\n",
    "        num_dense_units = trial.suggest_int('num_dense_units', 32, 512)\n",
    "\n",
    "        # Create the Keras model with the specified hyperparameters       \n",
    "        input_layer = Input(shape=(maxlen,), dtype=tf.int64)\n",
    "        embed = Embedding(numWords+1, output_dim=100, input_length=maxlen, weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "        conv = Conv1D(num_filters, filters_size, activation='relu')(embed)\n",
    "        max_pool = MaxPooling1D(pool_size=pool_size)(conv)\n",
    "        flatten = Flatten()(max_pool)\n",
    "        dense = Dense(num_dense_units, activation='relu')(flatten)\n",
    "        drop=Dropout(dropout_rate)(dense)    \n",
    "        out = Dense(len((pd.Series(y_train)).unique()), activation='softmax')(drop)\n",
    "         \n",
    "        model = Model(input_layer, out)\n",
    "            \n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "  \n",
    "        model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_Val, y_Val), verbose=0)\n",
    "        val_accuracy = model.evaluate(X_Val, y_Val, verbose=0)[1]\n",
    "        \n",
    "        return val_accuracy\n",
    "   \n",
    "    def train(self, batch_size, epochs):\n",
    "        # Create an Optuna study and run the hyperparameter search\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(self.objective, n_trials=30)\n",
    "\n",
    "        # Get the best hyperparameters and train the final model\n",
    "        best_params = study.best_params\n",
    "        best_learning_rate = best_params['learning_rate']\n",
    "        best_dropout_rate = best_params['dropout_rate']\n",
    "        best_num_filters = best_params['num_filters']\n",
    "        best_filters_size = best_params['filters_size']\n",
    "        best_pool_size = best_params['pool_size']\n",
    "        best_num_dense_units = best_params['num_dense_units']\n",
    "\n",
    "        input_layer = Input(shape=(maxlen,), dtype=tf.int64)\n",
    "        embed = Embedding(numWords+1, output_dim=100, input_length=maxlen, weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "        conv = Conv1D(best_num_filters, best_filters_size, activation='relu')(embed)\n",
    "        max_pool = MaxPooling1D(pool_size=best_pool_size)(conv)\n",
    "        flatten = Flatten()(max_pool)\n",
    "        dense = Dense(best_num_dense_units, activation='relu')(flatten)\n",
    "        drop=Dropout(best_dropout_rate)(dense)    \n",
    "        out = Dense(len((pd.Series(y_train)).unique()), activation='softmax')(drop)\n",
    "        \n",
    "        model = Model(input_layer, out)\n",
    "         \n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=best_learning_rate), metrics=['accuracy'])\n",
    "        model.summary()\n",
    "   \n",
    "        # reduceLoss = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001)\n",
    "        reduceLoss = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=1)\n",
    "   \n",
    "        model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduceLoss], validation_data=(X_Val,y_Val))\n",
    "\n",
    "        return model_history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \n",
    "    def prediction(self, model, X_test, y_test):\n",
    "      \n",
    "      pred = model.predict(X_test)\n",
    "      df_pred = pd.DataFrame(pred, columns=['tech1', 'tech2', 'tech3', 'tech4', 'tech5', 'tech6', 'tech7', 'tech8'])\n",
    "      pred = [i.argmax() for i in pred]\n",
    "      \n",
    "      accuracy = metrics.accuracy_score(y_test, pred)\n",
    "      precision = metrics.precision_score(y_test, pred, average='weighted')\n",
    "      recall = metrics.recall_score(y_test, pred, average='weighted')\n",
    "      f1score = metrics.f1_score(y_test, pred, average='weighted')\n",
    "      \n",
    "      print(\"Precision of Convolutional Neural Network model: \", precision)\n",
    "      print(\"Recall of Convolutional Neural Network model: \", recall)\n",
    "      print(\"F1-Score of Convolutional Neural Network model: \", f1score)\n",
    "      print(\"Accuracy of Convolutional Neural Network model:\", accuracy)\n",
    "      \n",
    "      print(classification_report(y_test, pred))\n",
    "      \n",
    "      return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 128, 100)          1000100   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              234496    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               25700     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 808       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,261,104\n",
      "Trainable params: 1,261,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "150/150 [==============================] - 91s 580ms/step - loss: 1.1962 - accuracy: 0.5777 - val_loss: 0.7070 - val_accuracy: 0.7848 - lr: 8.0000e-04\n",
      "Epoch 2/20\n",
      "150/150 [==============================] - 86s 573ms/step - loss: 0.6414 - accuracy: 0.7707 - val_loss: 0.4955 - val_accuracy: 0.8422 - lr: 8.0000e-04\n",
      "Epoch 3/20\n",
      "150/150 [==============================] - 87s 577ms/step - loss: 0.3110 - accuracy: 0.8919 - val_loss: 0.3937 - val_accuracy: 0.8770 - lr: 8.0000e-04\n",
      "Epoch 4/20\n",
      "150/150 [==============================] - 89s 593ms/step - loss: 0.1595 - accuracy: 0.9506 - val_loss: 0.2989 - val_accuracy: 0.9078 - lr: 8.0000e-04\n",
      "Epoch 5/20\n",
      "150/150 [==============================] - 89s 595ms/step - loss: 0.0637 - accuracy: 0.9817 - val_loss: 0.2827 - val_accuracy: 0.9098 - lr: 8.0000e-04\n",
      "Epoch 6/20\n",
      "150/150 [==============================] - 89s 590ms/step - loss: 0.0438 - accuracy: 0.9875 - val_loss: 0.2535 - val_accuracy: 0.9201 - lr: 8.0000e-04\n",
      "Epoch 7/20\n",
      "150/150 [==============================] - 88s 585ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.2984 - val_accuracy: 0.9119 - lr: 8.0000e-04\n",
      "Epoch 8/20\n",
      "150/150 [==============================] - 85s 564ms/step - loss: 0.0269 - accuracy: 0.9925 - val_loss: 0.2833 - val_accuracy: 0.9119 - lr: 6.4000e-04\n",
      "Epoch 9/20\n",
      "150/150 [==============================] - 86s 575ms/step - loss: 0.0190 - accuracy: 0.9949 - val_loss: 0.3396 - val_accuracy: 0.9160 - lr: 5.1200e-04\n",
      "Epoch 10/20\n",
      "150/150 [==============================] - 89s 591ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.3271 - val_accuracy: 0.9242 - lr: 4.0960e-04\n",
      "Epoch 11/20\n",
      "150/150 [==============================] - 90s 597ms/step - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.3623 - val_accuracy: 0.9160 - lr: 3.2768e-04\n",
      "Epoch 12/20\n",
      "150/150 [==============================] - 88s 589ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.3948 - val_accuracy: 0.9160 - lr: 2.6214e-04\n",
      "Epoch 13/20\n",
      "150/150 [==============================] - 90s 599ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.3946 - val_accuracy: 0.9180 - lr: 2.0972e-04\n",
      "Epoch 14/20\n",
      "150/150 [==============================] - 91s 608ms/step - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.3636 - val_accuracy: 0.9242 - lr: 1.6777e-04\n",
      "Epoch 15/20\n",
      "150/150 [==============================] - 95s 635ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.3750 - val_accuracy: 0.9242 - lr: 1.3422e-04\n",
      "Epoch 16/20\n",
      "150/150 [==============================] - 95s 631ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.3694 - val_accuracy: 0.9242 - lr: 1.0737e-04\n",
      "Epoch 17/20\n",
      "150/150 [==============================] - 94s 630ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.3666 - val_accuracy: 0.9283 - lr: 8.5899e-05\n",
      "Epoch 18/20\n",
      "150/150 [==============================] - 95s 633ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.3755 - val_accuracy: 0.9262 - lr: 6.8719e-05\n",
      "Epoch 19/20\n",
      "150/150 [==============================] - 94s 628ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.3795 - val_accuracy: 0.9283 - lr: 5.4976e-05\n",
      "Epoch 20/20\n",
      "150/150 [==============================] - 97s 647ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.3905 - val_accuracy: 0.9221 - lr: 4.3980e-05\n"
     ]
    }
   ],
   "source": [
    "CNNModel = CNNModel()\n",
    "model_history, model = CNNModel.train(X_train, y_train, X_Val, y_Val, embedding_matrix, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 3s 65ms/step\n",
      "Precision of Bi-Directional Long Short Term Memory model:  0.9292207646425427\n",
      "Recall of Bi-Directional Long Short Term Memory model:  0.9302707136997539\n",
      "F1-Score of Bi-Directional Long Short Term Memory model:  0.9261571231290466\n",
      "Accuracy of Bi-Directional Long Short Term Memory model: 0.9302707136997539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       798\n",
      "           1       0.96      0.90      0.92        48\n",
      "           2       0.80      0.87      0.84        38\n",
      "           3       0.84      0.81      0.83        47\n",
      "           4       1.00      1.00      1.00        59\n",
      "           5       0.87      0.80      0.84        41\n",
      "           6       0.79      0.94      0.86       133\n",
      "           7       0.75      0.38      0.51        55\n",
      "\n",
      "    accuracy                           0.93      1219\n",
      "   macro avg       0.87      0.83      0.85      1219\n",
      "weighted avg       0.93      0.93      0.93      1219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test model on unseen data and get prediction array\n",
    "Prediction = Prediction()\n",
    "all_predictions = Prediction.prediction(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN_model_BERT_OPTUNA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.to_csv('CNN_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
