{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Automatic_Ticket_Assignment.xlsx')\n",
    "df.columns=['ShortDescription','Description', 'Caller', 'AssignmentGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = df['AssignmentGroup'].value_counts()\n",
    "to_remove = group_counts[group_counts < 200].index\n",
    "df = df[~df['AssignmentGroup'].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Description'] = df.apply(lambda row: row['ShortDescription'] if pd.isna(row['Description']) else row['Description'], axis=1)\n",
    "df['ShortDescription'] = df.apply(lambda row: row['Description'] if pd.isna(row['ShortDescription']) else row['ShortDescription'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def apply_contractions(text):\n",
    "    new_phrase = []\n",
    "    for word in text.split():\n",
    "        new_phrase.append(contractions.fix(word))\n",
    "        \n",
    "    return ' '.join(new_phrase)\n",
    "        \n",
    "# Expanding Contractions in the reviews\n",
    "df['Description']=df['Description'].apply(lambda x:apply_contractions(x))\n",
    "df['ShortDescription']=df['ShortDescription'].apply(lambda x:apply_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleaner(text):\n",
    "    #converting to lowercase\n",
    "    newString = text.lower()\n",
    "    #removing links\n",
    "    newString = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', newString) \n",
    "    #fetching alphabetic characters\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    #removing stop words\n",
    "    tokens = [w for w in newString.split() if not w in stop_words] \n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        #removing short words\n",
    "        if len(i)>1:                                                 \n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['Description']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['Cleaned_Description'] = cleaned_text\n",
    "\n",
    "cleaned_text=[]\n",
    "for i in df['ShortDescription']:\n",
    "    cleaned_text.append(text_cleaner(i))\n",
    "\n",
    "df['ShortDescription'] = cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Consolidated_Text']=df['Cleaned_Description'] + ' ' + df['ShortDescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# function to genarate word tokens for tokenizers\n",
    "\n",
    "def tokenization_func(text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "list_for_sentence_word_tokens = []\n",
    "\n",
    "for sen in df.Consolidated_Text:\n",
    "    list_for_sentence_word_tokens.append(tokenization_func(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part Of Speech Tagging\n",
    "\n",
    "list_of_sen_with_part_of_speech_tagging = []\n",
    "for sen_list in list_for_sentence_word_tokens:\n",
    "    list_of_sen_with_part_of_speech_tagging.append(nltk.pos_tag(sen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Map POS tags to wordnet tags\n",
    "# This step is necessary because the lemmatizer requires WordNet tags instead of POS tags\n",
    "wordnet_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Normalize the words using lemmatization with the appropriate POS tags\n",
    "list_of_lemmatized_sen = []\n",
    "for sen_list in list_of_sen_with_part_of_speech_tagging:\n",
    "    lemmas = []\n",
    "    for word, pos in sen_list:\n",
    "        if pos[0] in wordnet_tags:\n",
    "            tag = wordnet_tags[pos[0]]\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            lemmas.append(lemma)\n",
    "        else:\n",
    "            lemmas.append(word)\n",
    "\n",
    "    # Join the lemmas back into a normalized sentence\n",
    "    normalized_sentence = \" \".join(lemmas)\n",
    "    # insert the lemmatized(normalized_sentence) sentence in a new list called list_of_lemmatized_sen\n",
    "    list_of_lemmatized_sen.append(normalized_sentence)\n",
    "\n",
    "df['Consolidated_Text'] = list_of_lemmatized_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['Consolidated_Text','AssignmentGroup']]\n",
    "new_df.columns=['text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.drop_duplicates(subset='text', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def labelencoder(dataframe) : \n",
    "  label_encoder = preprocessing.LabelEncoder() \n",
    "  dataframe= label_encoder.fit_transform(dataframe)\n",
    "  \n",
    "  return dataframe\n",
    "\n",
    "new_df['label'] = labelencoder(new_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10193 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [line.split(' ') for line in new_df['text']]\n",
    "word2vec = Word2Vec(sentences=sentences,min_count=1) # min_count=1 means that we are considering all the words in the corpus\n",
    "# This file will be used later to load the embeddings into memory for training a neural network\n",
    "# By default each word will be represented by a 100 dimensional vector\n",
    "word2vec.wv.save_word2vec_format('word2vec_vector.txt')\n",
    "\n",
    "# load the whole embedding\n",
    "embeddings_index = dict()\n",
    "f = open('word2vec_vector.txt')\n",
    "\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32') # converts the string vectors to float and store in a numpy array\n",
    "\tembeddings_index[word] = coefs # store the word and its corresponding vector in a dictionary\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, Conv1D, MaxPooling1D, GRU\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "\n",
    "maxlen = 100\n",
    "numWords=10000\n",
    "epochs = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedModelData:\n",
    "    \n",
    "    X_test=[]\n",
    "    y_test=[]\n",
    "    embedding_matrix=[]\n",
    "\n",
    "    # 1\n",
    "    def getData(self, dataframe):\n",
    "\n",
    "      X,y = self.tokenizeAndEmbedding(dataframe)\n",
    "      X_train, X_test, y_train, y_test, X_Val, y_Val = self.splitData(X,y)      \n",
    " \n",
    "      return X_train, X_test, y_train, y_test, X_Val, y_Val, self.embedding_matrix\n",
    "  \n",
    "    # 2\n",
    "    def tokenizeAndEmbedding(self,dataframe):\n",
    "\n",
    "      tokenizer,X = self.wordTokenizer(dataframe['text'])\n",
    "      y = np.asarray(dataframe['label'])\n",
    "      X = pad_sequences(X, maxlen = maxlen, padding='post', truncating='post') # ensure that all sequences have the same length                \n",
    "      self.embedding_matrix = np.zeros((numWords+1, 100))\n",
    "\n",
    "      for i,word in tokenizer.index_word.items():\n",
    "        if i<numWords+1: # we are taking only the first 9000 words\n",
    "          embedding_vector = embeddings_index.get(word)\n",
    "          if embedding_vector is not None:\n",
    "              self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "      return X,y\n",
    "    \n",
    "    # 3\n",
    "    def wordTokenizer(self, dataframe):\n",
    "      tokenizer = Tokenizer(num_words=numWords, lower=True, split=' ', char_level=False)\n",
    "      tokenizer.fit_on_texts(dataframe) # convert each word in the text into a unique integer ID\n",
    "      dataframe = tokenizer.texts_to_sequences(dataframe) # transform each text in dataframe into a sequence of integer indices\n",
    "      \n",
    "      with open('tokenizer.pickle', 'wb') as handle:\n",
    "          pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "      return tokenizer,dataframe\n",
    "\n",
    "    # 4\n",
    "    def splitData(self,X,y):\n",
    "\n",
    "      X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=10)     \n",
    "      X_train, X_Val, y_train, y_Val = train_test_split(X_train, y_train, test_size=0.1, random_state=10)\n",
    "      \n",
    "      #k-fold cross validation\n",
    "      # check classes count\n",
    "             \n",
    "      # smote = SMOTE(random_state=42)\n",
    "      # X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)     \n",
    "      # return X_train_resampled, self.X_test, y_train_resampled, self.y_test, X_Val, y_Val\n",
    "      return X_train, self.X_test, y_train, self.y_test, X_Val, y_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel:\n",
    "    \n",
    "    model= Model()\n",
    "    \n",
    "    def train(self, X_train, y_train, X_Val, y_Val, embedding_matrix, batch_size, epochs):\n",
    "        \n",
    "      input_layer = Input(shape=(maxlen,), dtype=tf.int64)\n",
    "      embed = Embedding(numWords+1, output_dim=100, input_length=maxlen, weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "      \n",
    "      gru = GRU(128, return_sequences=True)(embed)\n",
    "      gru = GRU(64)(gru)\n",
    "      \n",
    "      dense = Dense(128, activation='relu')(gru)\n",
    "      drop=Dropout(0.5)(dense)    \n",
    "      \n",
    "      out = Dense(len((pd.Series(y_train)).unique()), activation='softmax')(drop) \n",
    "      self.model = Model(input_layer, out)    \n",
    "      self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "      self.model.summary()\n",
    "  \n",
    "      reduceLoss = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=1, min_lr=0.0001)\n",
    "      \n",
    "      model_history = self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[reduceLoss], validation_data=(X_Val,y_Val))\n",
    "  \n",
    "      return model_history, self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \n",
    "    def prediction(self, model, X_test, y_test):\n",
    "      \n",
    "      pred = model.predict(X_test)\n",
    "      df_pred = pd.DataFrame(pred, columns=['tech1', 'tech2', 'tech3', 'tech4', 'tech5', 'tech6', 'tech7', 'tech8'])\n",
    "      pred = [i.argmax() for i in pred]\n",
    "      \n",
    "      accuracy = metrics.accuracy_score(y_test, pred)\n",
    "      precision = metrics.precision_score(y_test, pred, average='weighted')\n",
    "      recall = metrics.recall_score(y_test, pred, average='weighted')\n",
    "      f1score = metrics.f1_score(y_test, pred, average='weighted')\n",
    "      \n",
    "      print(\"Precision of Gated Recurrent Unit model: \", precision)\n",
    "      print(\"Recall of Gated Recurrent Unit model: \", recall)\n",
    "      print(\"F1-Score of Gated Recurrent Unit model: \", f1score)\n",
    "      print(\"Accuracy of Gated Recurrent Unit model:\", accuracy)\n",
    "      \n",
    "      return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get x_train, x_test, y_train, y_test, x_Val, y_Val, embedding_matrix to feed the model\n",
    "FeedModelData = FeedModelData()\n",
    "x_train, x_test, y_train, y_test, x_Val, y_Val, embedding_matrix = FeedModelData.getData(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 100, 100)          1000100   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100, 128)          88320     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,135,020\n",
      "Trainable params: 1,135,020\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "27/27 [==============================] - 22s 500ms/step - loss: 1.5661 - accuracy: 0.6907 - val_loss: 1.2469 - val_accuracy: 0.6765 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "27/27 [==============================] - 12s 441ms/step - loss: 1.2119 - accuracy: 0.6982 - val_loss: 1.2155 - val_accuracy: 0.6791 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "27/27 [==============================] - 12s 436ms/step - loss: 1.2047 - accuracy: 0.6999 - val_loss: 1.2039 - val_accuracy: 0.6818 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "27/27 [==============================] - 12s 435ms/step - loss: 1.1861 - accuracy: 0.7014 - val_loss: 1.2002 - val_accuracy: 0.6845 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "27/27 [==============================] - 12s 440ms/step - loss: 1.1793 - accuracy: 0.7032 - val_loss: 1.1916 - val_accuracy: 0.6898 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "27/27 [==============================] - 12s 442ms/step - loss: 1.1825 - accuracy: 0.7035 - val_loss: 1.1915 - val_accuracy: 0.6898 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "27/27 [==============================] - 12s 457ms/step - loss: 1.1775 - accuracy: 0.7050 - val_loss: 1.1972 - val_accuracy: 0.6898 - lr: 8.0000e-04\n",
      "Epoch 8/20\n",
      "27/27 [==============================] - 12s 437ms/step - loss: 1.1724 - accuracy: 0.7050 - val_loss: 1.1932 - val_accuracy: 0.6898 - lr: 6.4000e-04\n",
      "Epoch 9/20\n",
      "27/27 [==============================] - 12s 448ms/step - loss: 1.1774 - accuracy: 0.7053 - val_loss: 1.1879 - val_accuracy: 0.6872 - lr: 5.1200e-04\n",
      "Epoch 10/20\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 1.1709 - accuracy: 0.7056 - val_loss: 1.1902 - val_accuracy: 0.6925 - lr: 5.1200e-04\n",
      "Epoch 11/20\n",
      "27/27 [==============================] - 12s 437ms/step - loss: 1.1738 - accuracy: 0.7062 - val_loss: 1.1867 - val_accuracy: 0.6925 - lr: 4.0960e-04\n",
      "Epoch 12/20\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 1.1653 - accuracy: 0.7053 - val_loss: 1.1870 - val_accuracy: 0.6898 - lr: 4.0960e-04\n",
      "Epoch 13/20\n",
      "27/27 [==============================] - 12s 440ms/step - loss: 1.1712 - accuracy: 0.7062 - val_loss: 1.1876 - val_accuracy: 0.6898 - lr: 3.2768e-04\n",
      "Epoch 14/20\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 1.1654 - accuracy: 0.7062 - val_loss: 1.1864 - val_accuracy: 0.6898 - lr: 2.6214e-04\n",
      "Epoch 15/20\n",
      "27/27 [==============================] - 12s 442ms/step - loss: 1.1671 - accuracy: 0.7065 - val_loss: 1.1913 - val_accuracy: 0.6898 - lr: 2.6214e-04\n",
      "Epoch 16/20\n",
      "27/27 [==============================] - 12s 437ms/step - loss: 1.1668 - accuracy: 0.7059 - val_loss: 1.1864 - val_accuracy: 0.6898 - lr: 2.0972e-04\n",
      "Epoch 17/20\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 1.1718 - accuracy: 0.7062 - val_loss: 1.1869 - val_accuracy: 0.6898 - lr: 1.6777e-04\n",
      "Epoch 18/20\n",
      "27/27 [==============================] - 12s 446ms/step - loss: 1.1590 - accuracy: 0.7059 - val_loss: 1.1874 - val_accuracy: 0.6898 - lr: 1.3422e-04\n",
      "Epoch 19/20\n",
      "27/27 [==============================] - 12s 442ms/step - loss: 1.1699 - accuracy: 0.7053 - val_loss: 1.1872 - val_accuracy: 0.6898 - lr: 1.0737e-04\n",
      "Epoch 20/20\n",
      "27/27 [==============================] - 12s 446ms/step - loss: 1.1654 - accuracy: 0.7074 - val_loss: 1.1865 - val_accuracy: 0.6898 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# Define and train the model\n",
    "GRUModel = GRUModel()\n",
    "model_history, model = GRUModel.train(x_train, y_train, x_Val, y_Val, embedding_matrix, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 3s 56ms/step\n",
      "Precision of Gated Recurrent Unit model:  0.5478716179935692\n",
      "Recall of Gated Recurrent Unit model:  0.6923076923076923\n",
      "F1-Score of Gated Recurrent Unit model:  0.5855577716800268\n",
      "Accuracy of Gated Recurrent Unit model: 0.6923076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devesh Udhin\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# test model on unseen data and get prediction array\n",
    "Prediction = Prediction()\n",
    "all_predictions = Prediction.prediction(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('GRU_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.to_csv('GRU_model_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
